{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "keras.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BlueBug12/stock/blob/master/hw13_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfMKEx6zKSHG"
      },
      "source": [
        "# Keras\n",
        "\n",
        "[Keras](https://keras.io/) 是一種較 Tensorflow 更高階的深度學習框架，可以使用更少量的程式碼來建立深度學習模型，可以先熟悉 [Tensorflow 單元](/notebooks/unit/tensorflow/tenforflow.ipynb)後再來閱讀本單元。Keras 使用更低階的深度學習框架作為後端引擎，目前支援如 CNTK、Tensorflow、Theano 等知名框架。本單元將介紹 Keras 中 Model 與 Layer 的用法，並實作一個圖片分類器。\n",
        "\n",
        "## 1. Model & Layer\n",
        "\n",
        "在 Keras，可以宣告一個 [Model](https://keras.io/models/about-keras-models/) 物件，並透過加入一層一層的 [Layer](https://keras.io/layers/about-keras-layers/) 來建構一個神經網路，神經網路的運算(例如訓練)都可以透過該 Model 物件來操作。下方程式區段使用了 Keras 中常見的 [Sequential Model](https://keras.io/models/sequential/)，並加入了四種 Layer：\n",
        "\n",
        "1. [Convolutional Layer](https://keras.io/layers/convolutional/)：卷積層在影像、圖片應用上，表現比全連結層(Keras 的 Dense Layer 更為優異)，參考[卷積神經網絡介紹](https://medium.com/@yehjames/4f8249d65d4f)。\n",
        "2. [Pooling Layer](https://keras.io/layers/pooling/#maxpooling2d)：池化層的工作是降採樣(down sampling)，以下方程式區段使用的 MaxPooling 為例，將每個 2x2 降採樣為該區域的最大值。\n",
        "3. [Flatten Layer](https://keras.io/layers/core/#flatten)：將原本多維度的資料拉平成一維，目的是讓前一層的輸出可以接到下一層(通常是全連接層)的輸入。\n",
        "4. [Dense Layer](https://keras.io/layers/core/#dense)：全連結層。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeSIMwJFKSHH",
        "outputId": "d3bb74ab-6a31-4d28-bdee-0186f3c9cc39"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
        "\n",
        "model = Sequential() # Declare a sequential model\n",
        "\n",
        "# Add a 2D convolutional layer with 64 nodes, a 3x3 filter and relu as avtivation function\n",
        "# After this layer, `model.output_shape` is (None, 62, 62, 64)\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
        "\n",
        "# Add a 2D max pooling layer that pools the maximun value every 2x2 area\n",
        "# After this layer, `model.output_shape` is (None, 31, 31, 64)\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add a flatten layer\n",
        "# After this layer, `model.output_shape` is (None, 61504)\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a dense layer with 32 nodes and sigmoid as activation function\n",
        "# After this layer, `model.output_shape` is (None, 32)\n",
        "model.add(Dense(32, activation='sigmoid'))\n",
        "\n",
        "# See `model`\n",
        "model.summary() "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 62, 62, 64)        1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 31, 31, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 61504)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                1968160   \n",
            "=================================================================\n",
            "Total params: 1,969,952\n",
            "Trainable params: 1,969,952\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPr4u6ADKSHI"
      },
      "source": [
        "## 2. CIFAR-10\n",
        "\n",
        "CIFAR 的全名為 Canadian Institute for Advanced Research，是由加拿大政府出資並由多位科學家、工程師收集而成的圖片資料庫。[CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html) 包含 60000 張 32x32x3 的 RGB 彩色圖片，其中 50000 張為訓練資料，10000 張為測試資料。CIFAR-10 有 10 種類別，0~9 分別對應為：\n",
        "\n",
        "```\n",
        "airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
        "```\n",
        "\n",
        "![CIFAR-10](./cifar_10.png)\n",
        "\n",
        "Keras 提供[整理好的 CIFAR-10 資料](https://keras.io/datasets/#cifar10-small-image-classification)，只要透過 `import` 就可以拿到對應的訓練與測試資料。用法如下："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "Yjtheix7KSHJ",
        "outputId": "258759cb-f154-4536-bd21-3c34eabf8dda"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# see the data shape\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)\n",
        "print()\n",
        "\n",
        "# show the i-th sample of the cifar-10 training set, try a different `i`\n",
        "i = 0\n",
        "plt.imshow(x_train[i])\n",
        "print('The label of training sample %d is %s.' % (i, y_train[i]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n",
            "(50000, 32, 32, 3) (50000, 1)\n",
            "(10000, 32, 32, 3) (10000, 1)\n",
            "\n",
            "The label of training sample 0 is [6].\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZsasu22KSHJ"
      },
      "source": [
        "## 3. 範例模型\n",
        "\n",
        "以下使用 Keras 實作兩種深度學習模型來進行 CIFAR-10 圖片分類。其中 DNN 只使用全連接層，而 CNN 多使用了卷積層。相較於 [Tensorflow 單元](/notebooks/unit/tensorflow/tenforflow.ipynb)的 MNIST 資料，CIFAR-10 的圖片比較複雜且為彩色，更能發揮卷積層的效果。也因此相較於 DNN，CNN 應該更容易得到好的結果。注意比較以下兩個程式區段，使用 CNN 時不需要將圖片 reshape 為一維向量。當然，CNN 也能處理 reshape 過的一維向量。CNN 的初學者可以參考以下連結：\n",
        "\n",
        "* [A Beginner's Guide To Understanding Convolutional Neural Networks](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)\n",
        "* [深度學習(2)--使用Tensorflow實作卷積神經網路(Convolutional neural network，CNN)](http://arbu00.blogspot.tw/2017/03/2-tensorflowconvolutional-neural.html)\n",
        "\n",
        "![Convolutional Neural Network](https://adeshpande3.github.io/assets/Cover.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8aHoc-8KSHJ",
        "outputId": "efec8d9f-6a16-4d9d-a83c-e3a24bdb6ff3"
      },
      "source": [
        "# DNN\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
        "\n",
        "x_tr_dnn = x_train[:10000].astype('float32')\n",
        "x_te_dnn = x_test.astype('float32')\n",
        "\n",
        "# note that the CNN version does not need to reshape the input\n",
        "x_tr_dnn = x_tr_dnn.reshape(-1, 3072)\n",
        "x_te_dnn = x_te_dnn.reshape(-1, 3072)\n",
        "\n",
        "# normalize\n",
        "x_tr_dnn /= 255\n",
        "x_te_dnn /= 255\n",
        "\n",
        "# one-hot encoding\n",
        "y_tr_dnn = to_categorical(y_train[:10000], num_classes=10)\n",
        "y_te_dnn = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# options\n",
        "epochs = 20\n",
        "batch_size = 128 \n",
        "learning_rate = 0.001\n",
        "\n",
        "# model\n",
        "model = Sequential()\n",
        "model.add(Dense(100, activation='relu', input_shape=(3072,)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "# train\n",
        "model.fit(x_tr_dnn, y_tr_dnn, batch_size=batch_size, epochs=epochs, shuffle=True, validation_data=(x_te_dnn, y_te_dnn))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "79/79 [==============================] - 1s 7ms/step - loss: 2.1739 - accuracy: 0.2080 - val_loss: 2.0037 - val_accuracy: 0.2761\n",
            "Epoch 2/20\n",
            "79/79 [==============================] - 1s 7ms/step - loss: 1.9803 - accuracy: 0.2853 - val_loss: 1.9444 - val_accuracy: 0.2993\n",
            "Epoch 3/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.9128 - accuracy: 0.3151 - val_loss: 1.8957 - val_accuracy: 0.3260\n",
            "Epoch 4/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.8442 - accuracy: 0.3476 - val_loss: 1.8586 - val_accuracy: 0.3384\n",
            "Epoch 5/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.7992 - accuracy: 0.3655 - val_loss: 1.8229 - val_accuracy: 0.3558\n",
            "Epoch 6/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.7667 - accuracy: 0.3773 - val_loss: 1.8281 - val_accuracy: 0.3514\n",
            "Epoch 7/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.7484 - accuracy: 0.3805 - val_loss: 1.8121 - val_accuracy: 0.3513\n",
            "Epoch 8/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.7242 - accuracy: 0.3911 - val_loss: 1.8237 - val_accuracy: 0.3566\n",
            "Epoch 9/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.7051 - accuracy: 0.3993 - val_loss: 1.7712 - val_accuracy: 0.3750\n",
            "Epoch 10/20\n",
            "79/79 [==============================] - 0s 5ms/step - loss: 1.6815 - accuracy: 0.4057 - val_loss: 1.7557 - val_accuracy: 0.3780\n",
            "Epoch 11/20\n",
            "79/79 [==============================] - 0s 5ms/step - loss: 1.6702 - accuracy: 0.4112 - val_loss: 1.8020 - val_accuracy: 0.3622\n",
            "Epoch 12/20\n",
            "79/79 [==============================] - 0s 5ms/step - loss: 1.6693 - accuracy: 0.4155 - val_loss: 1.7835 - val_accuracy: 0.3657\n",
            "Epoch 13/20\n",
            "79/79 [==============================] - 0s 5ms/step - loss: 1.6447 - accuracy: 0.4222 - val_loss: 1.7687 - val_accuracy: 0.3773\n",
            "Epoch 14/20\n",
            "79/79 [==============================] - 0s 5ms/step - loss: 1.6357 - accuracy: 0.4245 - val_loss: 1.7654 - val_accuracy: 0.3771\n",
            "Epoch 15/20\n",
            "79/79 [==============================] - 0s 5ms/step - loss: 1.6279 - accuracy: 0.4317 - val_loss: 1.8005 - val_accuracy: 0.3612\n",
            "Epoch 16/20\n",
            "79/79 [==============================] - 0s 5ms/step - loss: 1.6250 - accuracy: 0.4325 - val_loss: 1.7338 - val_accuracy: 0.3835\n",
            "Epoch 17/20\n",
            "79/79 [==============================] - 0s 5ms/step - loss: 1.5939 - accuracy: 0.4418 - val_loss: 1.7502 - val_accuracy: 0.3758\n",
            "Epoch 18/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.5982 - accuracy: 0.4381 - val_loss: 1.7280 - val_accuracy: 0.3855\n",
            "Epoch 19/20\n",
            "79/79 [==============================] - 0s 6ms/step - loss: 1.5813 - accuracy: 0.4505 - val_loss: 1.7146 - val_accuracy: 0.3903\n",
            "Epoch 20/20\n",
            "79/79 [==============================] - 0s 5ms/step - loss: 1.5838 - accuracy: 0.4445 - val_loss: 1.8032 - val_accuracy: 0.3661\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4826a9f518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4hx2ylaKSHJ",
        "outputId": "910b2c8e-37f8-4c06-9f4b-4f1e77924b78"
      },
      "source": [
        "# CNN\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "x_tr_cnn = x_train[:50000].astype('float32')\n",
        "x_te_cnn = x_test.astype('float32')\n",
        "\n",
        "# normalize\n",
        "x_tr_cnn /= 255\n",
        "x_te_cnn /= 255\n",
        "\n",
        "# one-hot encoding\n",
        "y_tr_cnn = to_categorical(y_train[:50000], num_classes=10)\n",
        "y_te_cnn = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# options\n",
        "epochs = 200\n",
        "batch_size = 128 \n",
        "learning_rate = 0.001\n",
        "\n",
        "# model\n",
        "model = Sequential()\n",
        "\n",
        "# the input shape for cifar-10 is (32, 32, 3)\n",
        "# use `Conv2D(#neurons, (filter_size))` to add convolutionary layers\n",
        "model.add(Conv2D(64, (3, 3), input_shape=(32, 32, 3),padding='same', activation='relu'))\n",
        "\n",
        "# use `MaxPooling2D()` to add pooling layers\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2),strides=(2,2)))\n",
        "model.add(Conv2D(128, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(128, (3, 3), padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2),strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(256, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2),strides=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(256, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2),strides=(2,2)))\n",
        "'''\n",
        "model.add(Conv2D(512, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2),strides=(2,2)))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same',activation='relu'))\n",
        "model.add(Conv2D(512, (3, 3), padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2),strides=(2,2)))\n",
        "'''\n",
        "#model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# TODO: add more convolutionary and/or pooling layers here\n",
        "\n",
        "# in practice, fully-connected layers are added after convolutionary and pooling ones \n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "# train\n",
        "callback = EarlyStopping(monitor=\"val_accuracy\", patience=10, verbose=1, mode=\"auto\")\n",
        "#model.fit(x_tr_cnn, y_tr_cnn, batch_size=batch_size, epochs=epochs, shuffle=True, validation_data=(x_te_cnn, y_te_cnn), callbacks=[callback])\n",
        "model.fit(x_tr_cnn, y_tr_cnn, batch_size=batch_size, epochs=epochs, shuffle=True, validation_data=(x_te_cnn, y_te_cnn))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "  1/391 [..............................] - ETA: 11s - loss: 2.3018 - accuracy: 0.1016WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0173s vs `on_train_batch_end` time: 0.0300s). Check your callbacks.\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 2.3025 - accuracy: 0.1085 - val_loss: 2.3023 - val_accuracy: 0.1005\n",
            "Epoch 2/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 2.3015 - accuracy: 0.1115 - val_loss: 2.3014 - val_accuracy: 0.1035\n",
            "Epoch 3/200\n",
            "391/391 [==============================] - 13s 34ms/step - loss: 2.2981 - accuracy: 0.1359 - val_loss: 2.2951 - val_accuracy: 0.1737\n",
            "Epoch 4/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 2.2497 - accuracy: 0.1492 - val_loss: 2.1698 - val_accuracy: 0.1662\n",
            "Epoch 5/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 2.1352 - accuracy: 0.1969 - val_loss: 2.0995 - val_accuracy: 0.2212\n",
            "Epoch 6/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 2.0538 - accuracy: 0.2329 - val_loss: 1.9868 - val_accuracy: 0.2716\n",
            "Epoch 7/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.9985 - accuracy: 0.2531 - val_loss: 1.9356 - val_accuracy: 0.2730\n",
            "Epoch 8/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.9328 - accuracy: 0.2758 - val_loss: 1.8459 - val_accuracy: 0.3065\n",
            "Epoch 9/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.8647 - accuracy: 0.3020 - val_loss: 2.0215 - val_accuracy: 0.2582\n",
            "Epoch 10/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.7986 - accuracy: 0.3230 - val_loss: 1.7293 - val_accuracy: 0.3533\n",
            "Epoch 11/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.7471 - accuracy: 0.3424 - val_loss: 1.9195 - val_accuracy: 0.2928\n",
            "Epoch 12/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.7013 - accuracy: 0.3622 - val_loss: 1.7283 - val_accuracy: 0.3613\n",
            "Epoch 13/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.6618 - accuracy: 0.3781 - val_loss: 1.7258 - val_accuracy: 0.3618\n",
            "Epoch 14/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.6215 - accuracy: 0.3925 - val_loss: 1.5798 - val_accuracy: 0.4135\n",
            "Epoch 15/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.5856 - accuracy: 0.4085 - val_loss: 1.7375 - val_accuracy: 0.3679\n",
            "Epoch 16/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.5603 - accuracy: 0.4174 - val_loss: 1.8047 - val_accuracy: 0.3583\n",
            "Epoch 17/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.5164 - accuracy: 0.4352 - val_loss: 1.4935 - val_accuracy: 0.4435\n",
            "Epoch 18/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.4821 - accuracy: 0.4496 - val_loss: 1.5386 - val_accuracy: 0.4279\n",
            "Epoch 19/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.4425 - accuracy: 0.4653 - val_loss: 1.3705 - val_accuracy: 0.4927\n",
            "Epoch 20/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.4097 - accuracy: 0.4763 - val_loss: 1.4055 - val_accuracy: 0.4817\n",
            "Epoch 21/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.3773 - accuracy: 0.4913 - val_loss: 1.3718 - val_accuracy: 0.4977\n",
            "Epoch 22/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.3425 - accuracy: 0.5069 - val_loss: 1.4875 - val_accuracy: 0.4657\n",
            "Epoch 23/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.3094 - accuracy: 0.5193 - val_loss: 1.3301 - val_accuracy: 0.5197\n",
            "Epoch 24/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.2793 - accuracy: 0.5337 - val_loss: 1.2732 - val_accuracy: 0.5427\n",
            "Epoch 25/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.2493 - accuracy: 0.5422 - val_loss: 1.2411 - val_accuracy: 0.5487\n",
            "Epoch 26/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.2265 - accuracy: 0.5539 - val_loss: 1.2275 - val_accuracy: 0.5541\n",
            "Epoch 27/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.1975 - accuracy: 0.5637 - val_loss: 1.1643 - val_accuracy: 0.5760\n",
            "Epoch 28/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.1731 - accuracy: 0.5755 - val_loss: 1.1150 - val_accuracy: 0.6002\n",
            "Epoch 29/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.1462 - accuracy: 0.5816 - val_loss: 1.1203 - val_accuracy: 0.5972\n",
            "Epoch 30/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.1208 - accuracy: 0.5920 - val_loss: 1.0555 - val_accuracy: 0.6200\n",
            "Epoch 31/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.1009 - accuracy: 0.6009 - val_loss: 1.1418 - val_accuracy: 0.5939\n",
            "Epoch 32/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.0774 - accuracy: 0.6110 - val_loss: 1.1483 - val_accuracy: 0.5944\n",
            "Epoch 33/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.0538 - accuracy: 0.6177 - val_loss: 1.0786 - val_accuracy: 0.6191\n",
            "Epoch 34/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.0354 - accuracy: 0.6246 - val_loss: 1.0343 - val_accuracy: 0.6338\n",
            "Epoch 35/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 1.0133 - accuracy: 0.6341 - val_loss: 1.1012 - val_accuracy: 0.6068\n",
            "Epoch 36/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.9867 - accuracy: 0.6435 - val_loss: 1.0080 - val_accuracy: 0.6432\n",
            "Epoch 37/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.9715 - accuracy: 0.6495 - val_loss: 0.9655 - val_accuracy: 0.6591\n",
            "Epoch 38/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.9509 - accuracy: 0.6554 - val_loss: 0.9521 - val_accuracy: 0.6659\n",
            "Epoch 39/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.9286 - accuracy: 0.6658 - val_loss: 0.9184 - val_accuracy: 0.6778\n",
            "Epoch 40/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.9106 - accuracy: 0.6741 - val_loss: 0.8879 - val_accuracy: 0.6877\n",
            "Epoch 41/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.8904 - accuracy: 0.6791 - val_loss: 0.9004 - val_accuracy: 0.6830\n",
            "Epoch 42/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.8690 - accuracy: 0.6888 - val_loss: 0.9771 - val_accuracy: 0.6491\n",
            "Epoch 43/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.8512 - accuracy: 0.6954 - val_loss: 0.8925 - val_accuracy: 0.6846\n",
            "Epoch 44/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.8326 - accuracy: 0.7006 - val_loss: 0.8857 - val_accuracy: 0.6879\n",
            "Epoch 45/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.8141 - accuracy: 0.7118 - val_loss: 0.8583 - val_accuracy: 0.7000\n",
            "Epoch 46/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.8002 - accuracy: 0.7158 - val_loss: 0.8359 - val_accuracy: 0.7066\n",
            "Epoch 47/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.7818 - accuracy: 0.7223 - val_loss: 0.8811 - val_accuracy: 0.6938\n",
            "Epoch 48/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.7639 - accuracy: 0.7285 - val_loss: 0.7664 - val_accuracy: 0.7322\n",
            "Epoch 49/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.7510 - accuracy: 0.7321 - val_loss: 0.8417 - val_accuracy: 0.7078\n",
            "Epoch 50/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.7350 - accuracy: 0.7381 - val_loss: 0.8332 - val_accuracy: 0.7075\n",
            "Epoch 51/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.7157 - accuracy: 0.7464 - val_loss: 0.7907 - val_accuracy: 0.7226\n",
            "Epoch 52/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.7080 - accuracy: 0.7496 - val_loss: 0.7708 - val_accuracy: 0.7345\n",
            "Epoch 53/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.6925 - accuracy: 0.7534 - val_loss: 0.8050 - val_accuracy: 0.7212\n",
            "Epoch 54/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.6832 - accuracy: 0.7579 - val_loss: 0.7590 - val_accuracy: 0.7339\n",
            "Epoch 55/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.6714 - accuracy: 0.7619 - val_loss: 0.7147 - val_accuracy: 0.7512\n",
            "Epoch 56/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.6586 - accuracy: 0.7667 - val_loss: 0.7208 - val_accuracy: 0.7508\n",
            "Epoch 57/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.6473 - accuracy: 0.7696 - val_loss: 0.7709 - val_accuracy: 0.7348\n",
            "Epoch 58/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.6391 - accuracy: 0.7733 - val_loss: 0.6930 - val_accuracy: 0.7612\n",
            "Epoch 59/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.6215 - accuracy: 0.7798 - val_loss: 0.6723 - val_accuracy: 0.7711\n",
            "Epoch 60/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.6174 - accuracy: 0.7812 - val_loss: 0.6750 - val_accuracy: 0.7698\n",
            "Epoch 61/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.6001 - accuracy: 0.7880 - val_loss: 0.6885 - val_accuracy: 0.7674\n",
            "Epoch 62/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.5928 - accuracy: 0.7888 - val_loss: 0.6886 - val_accuracy: 0.7624\n",
            "Epoch 63/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.5828 - accuracy: 0.7933 - val_loss: 0.6653 - val_accuracy: 0.7737\n",
            "Epoch 64/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.5764 - accuracy: 0.7962 - val_loss: 0.7264 - val_accuracy: 0.7588\n",
            "Epoch 65/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.5653 - accuracy: 0.7997 - val_loss: 0.7490 - val_accuracy: 0.7561\n",
            "Epoch 66/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.5616 - accuracy: 0.7998 - val_loss: 0.6189 - val_accuracy: 0.7844\n",
            "Epoch 67/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.5460 - accuracy: 0.8074 - val_loss: 0.6500 - val_accuracy: 0.7803\n",
            "Epoch 68/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.5388 - accuracy: 0.8079 - val_loss: 0.6485 - val_accuracy: 0.7834\n",
            "Epoch 69/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.5291 - accuracy: 0.8129 - val_loss: 0.6241 - val_accuracy: 0.7901\n",
            "Epoch 70/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.5240 - accuracy: 0.8143 - val_loss: 0.6583 - val_accuracy: 0.7834\n",
            "Epoch 71/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.5180 - accuracy: 0.8178 - val_loss: 0.6209 - val_accuracy: 0.7900\n",
            "Epoch 72/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.5054 - accuracy: 0.8213 - val_loss: 0.7422 - val_accuracy: 0.7495\n",
            "Epoch 73/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.5005 - accuracy: 0.8219 - val_loss: 0.6038 - val_accuracy: 0.7989\n",
            "Epoch 74/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.4910 - accuracy: 0.8256 - val_loss: 0.6391 - val_accuracy: 0.7859\n",
            "Epoch 75/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.4839 - accuracy: 0.8301 - val_loss: 0.6551 - val_accuracy: 0.7859\n",
            "Epoch 76/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.4760 - accuracy: 0.8303 - val_loss: 0.6054 - val_accuracy: 0.7955\n",
            "Epoch 77/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.4701 - accuracy: 0.8335 - val_loss: 0.6206 - val_accuracy: 0.7930\n",
            "Epoch 78/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.4592 - accuracy: 0.8365 - val_loss: 0.5885 - val_accuracy: 0.8042\n",
            "Epoch 79/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.4552 - accuracy: 0.8392 - val_loss: 0.5928 - val_accuracy: 0.8009\n",
            "Epoch 80/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.4475 - accuracy: 0.8425 - val_loss: 0.6315 - val_accuracy: 0.7988\n",
            "Epoch 81/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.4420 - accuracy: 0.8439 - val_loss: 0.5938 - val_accuracy: 0.8036\n",
            "Epoch 82/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.4342 - accuracy: 0.8449 - val_loss: 0.6374 - val_accuracy: 0.7940\n",
            "Epoch 83/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.4316 - accuracy: 0.8457 - val_loss: 0.6874 - val_accuracy: 0.7728\n",
            "Epoch 84/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.4239 - accuracy: 0.8489 - val_loss: 0.5575 - val_accuracy: 0.8149\n",
            "Epoch 85/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.4192 - accuracy: 0.8510 - val_loss: 0.5749 - val_accuracy: 0.8128\n",
            "Epoch 86/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.4105 - accuracy: 0.8558 - val_loss: 0.5680 - val_accuracy: 0.8169\n",
            "Epoch 87/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.4063 - accuracy: 0.8562 - val_loss: 0.6076 - val_accuracy: 0.8050\n",
            "Epoch 88/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3991 - accuracy: 0.8577 - val_loss: 0.5765 - val_accuracy: 0.8166\n",
            "Epoch 89/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3933 - accuracy: 0.8603 - val_loss: 0.5652 - val_accuracy: 0.8163\n",
            "Epoch 90/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3823 - accuracy: 0.8647 - val_loss: 0.5774 - val_accuracy: 0.8117\n",
            "Epoch 91/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3799 - accuracy: 0.8667 - val_loss: 0.5511 - val_accuracy: 0.8196\n",
            "Epoch 92/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3728 - accuracy: 0.8685 - val_loss: 0.6406 - val_accuracy: 0.8064\n",
            "Epoch 93/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3703 - accuracy: 0.8665 - val_loss: 0.5641 - val_accuracy: 0.8187\n",
            "Epoch 94/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3612 - accuracy: 0.8718 - val_loss: 0.6092 - val_accuracy: 0.8116\n",
            "Epoch 95/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3564 - accuracy: 0.8724 - val_loss: 0.5712 - val_accuracy: 0.8142\n",
            "Epoch 96/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3516 - accuracy: 0.8750 - val_loss: 0.5818 - val_accuracy: 0.8133\n",
            "Epoch 97/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3463 - accuracy: 0.8771 - val_loss: 0.5931 - val_accuracy: 0.8175\n",
            "Epoch 98/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3475 - accuracy: 0.8760 - val_loss: 0.6270 - val_accuracy: 0.8081\n",
            "Epoch 99/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3383 - accuracy: 0.8800 - val_loss: 0.5738 - val_accuracy: 0.8217\n",
            "Epoch 100/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3333 - accuracy: 0.8823 - val_loss: 0.6053 - val_accuracy: 0.8086\n",
            "Epoch 101/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3239 - accuracy: 0.8853 - val_loss: 0.6010 - val_accuracy: 0.8134\n",
            "Epoch 102/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3215 - accuracy: 0.8855 - val_loss: 0.6599 - val_accuracy: 0.8013\n",
            "Epoch 103/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3162 - accuracy: 0.8880 - val_loss: 0.5856 - val_accuracy: 0.8165\n",
            "Epoch 104/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3160 - accuracy: 0.8869 - val_loss: 0.5562 - val_accuracy: 0.8288\n",
            "Epoch 105/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3089 - accuracy: 0.8919 - val_loss: 0.5967 - val_accuracy: 0.8200\n",
            "Epoch 106/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3016 - accuracy: 0.8934 - val_loss: 0.5695 - val_accuracy: 0.8212\n",
            "Epoch 107/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.3008 - accuracy: 0.8942 - val_loss: 0.5813 - val_accuracy: 0.8196\n",
            "Epoch 108/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2981 - accuracy: 0.8940 - val_loss: 0.5721 - val_accuracy: 0.8280\n",
            "Epoch 109/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2916 - accuracy: 0.8965 - val_loss: 0.5702 - val_accuracy: 0.8253\n",
            "Epoch 110/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2863 - accuracy: 0.8972 - val_loss: 0.5779 - val_accuracy: 0.8202\n",
            "Epoch 111/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2848 - accuracy: 0.8986 - val_loss: 0.5773 - val_accuracy: 0.8266\n",
            "Epoch 112/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2801 - accuracy: 0.8995 - val_loss: 0.5546 - val_accuracy: 0.8345\n",
            "Epoch 113/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2698 - accuracy: 0.9040 - val_loss: 0.6374 - val_accuracy: 0.8174\n",
            "Epoch 114/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2691 - accuracy: 0.9035 - val_loss: 0.5765 - val_accuracy: 0.8291\n",
            "Epoch 115/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2655 - accuracy: 0.9068 - val_loss: 0.5627 - val_accuracy: 0.8317\n",
            "Epoch 116/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2638 - accuracy: 0.9051 - val_loss: 0.5780 - val_accuracy: 0.8273\n",
            "Epoch 117/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2556 - accuracy: 0.9082 - val_loss: 0.5809 - val_accuracy: 0.8267\n",
            "Epoch 118/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2505 - accuracy: 0.9107 - val_loss: 0.5533 - val_accuracy: 0.8336\n",
            "Epoch 119/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2461 - accuracy: 0.9132 - val_loss: 0.6414 - val_accuracy: 0.8157\n",
            "Epoch 120/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2469 - accuracy: 0.9123 - val_loss: 0.6211 - val_accuracy: 0.8203\n",
            "Epoch 121/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2412 - accuracy: 0.9143 - val_loss: 0.5934 - val_accuracy: 0.8315\n",
            "Epoch 122/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2416 - accuracy: 0.9141 - val_loss: 0.5834 - val_accuracy: 0.8290\n",
            "Epoch 123/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2323 - accuracy: 0.9178 - val_loss: 0.5598 - val_accuracy: 0.8351\n",
            "Epoch 124/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2262 - accuracy: 0.9180 - val_loss: 0.5995 - val_accuracy: 0.8261\n",
            "Epoch 125/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2253 - accuracy: 0.9197 - val_loss: 0.5483 - val_accuracy: 0.8379\n",
            "Epoch 126/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2256 - accuracy: 0.9198 - val_loss: 0.5748 - val_accuracy: 0.8323\n",
            "Epoch 127/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2200 - accuracy: 0.9221 - val_loss: 0.5826 - val_accuracy: 0.8339\n",
            "Epoch 128/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2157 - accuracy: 0.9227 - val_loss: 0.5957 - val_accuracy: 0.8343\n",
            "Epoch 129/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2197 - accuracy: 0.9233 - val_loss: 0.5782 - val_accuracy: 0.8376\n",
            "Epoch 130/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2142 - accuracy: 0.9240 - val_loss: 0.6071 - val_accuracy: 0.8293\n",
            "Epoch 131/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2050 - accuracy: 0.9260 - val_loss: 0.6010 - val_accuracy: 0.8315\n",
            "Epoch 132/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.2032 - accuracy: 0.9260 - val_loss: 0.5946 - val_accuracy: 0.8399\n",
            "Epoch 133/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1995 - accuracy: 0.9286 - val_loss: 0.5743 - val_accuracy: 0.8410\n",
            "Epoch 134/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1998 - accuracy: 0.9293 - val_loss: 0.5876 - val_accuracy: 0.8392\n",
            "Epoch 135/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1945 - accuracy: 0.9296 - val_loss: 0.6282 - val_accuracy: 0.8304\n",
            "Epoch 136/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1892 - accuracy: 0.9314 - val_loss: 0.6529 - val_accuracy: 0.8292\n",
            "Epoch 137/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1885 - accuracy: 0.9330 - val_loss: 0.6356 - val_accuracy: 0.8276\n",
            "Epoch 138/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1843 - accuracy: 0.9337 - val_loss: 0.6111 - val_accuracy: 0.8399\n",
            "Epoch 139/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1867 - accuracy: 0.9329 - val_loss: 0.6517 - val_accuracy: 0.8285\n",
            "Epoch 140/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1776 - accuracy: 0.9357 - val_loss: 0.5943 - val_accuracy: 0.8382\n",
            "Epoch 141/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1748 - accuracy: 0.9375 - val_loss: 0.6329 - val_accuracy: 0.8278\n",
            "Epoch 142/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1760 - accuracy: 0.9368 - val_loss: 0.6403 - val_accuracy: 0.8305\n",
            "Epoch 143/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1723 - accuracy: 0.9379 - val_loss: 0.7120 - val_accuracy: 0.8193\n",
            "Epoch 144/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1660 - accuracy: 0.9407 - val_loss: 0.6090 - val_accuracy: 0.8362\n",
            "Epoch 145/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1682 - accuracy: 0.9391 - val_loss: 0.5879 - val_accuracy: 0.8424\n",
            "Epoch 146/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1618 - accuracy: 0.9420 - val_loss: 0.7100 - val_accuracy: 0.8208\n",
            "Epoch 147/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1595 - accuracy: 0.9432 - val_loss: 0.6473 - val_accuracy: 0.8327\n",
            "Epoch 148/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1582 - accuracy: 0.9425 - val_loss: 0.6219 - val_accuracy: 0.8400\n",
            "Epoch 149/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1564 - accuracy: 0.9433 - val_loss: 0.5917 - val_accuracy: 0.8396\n",
            "Epoch 150/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1552 - accuracy: 0.9449 - val_loss: 0.6128 - val_accuracy: 0.8444\n",
            "Epoch 151/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1567 - accuracy: 0.9437 - val_loss: 0.6460 - val_accuracy: 0.8328\n",
            "Epoch 152/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1496 - accuracy: 0.9471 - val_loss: 0.6224 - val_accuracy: 0.8439\n",
            "Epoch 153/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1468 - accuracy: 0.9470 - val_loss: 0.6451 - val_accuracy: 0.8373\n",
            "Epoch 154/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1464 - accuracy: 0.9471 - val_loss: 0.6893 - val_accuracy: 0.8315\n",
            "Epoch 155/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1378 - accuracy: 0.9506 - val_loss: 0.6573 - val_accuracy: 0.8410\n",
            "Epoch 156/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1410 - accuracy: 0.9485 - val_loss: 0.6387 - val_accuracy: 0.8372\n",
            "Epoch 157/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1393 - accuracy: 0.9495 - val_loss: 0.6507 - val_accuracy: 0.8360\n",
            "Epoch 158/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1331 - accuracy: 0.9518 - val_loss: 0.6163 - val_accuracy: 0.8433\n",
            "Epoch 159/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1371 - accuracy: 0.9503 - val_loss: 0.6279 - val_accuracy: 0.8439\n",
            "Epoch 160/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1319 - accuracy: 0.9528 - val_loss: 0.6235 - val_accuracy: 0.8479\n",
            "Epoch 161/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1315 - accuracy: 0.9523 - val_loss: 0.6513 - val_accuracy: 0.8377\n",
            "Epoch 162/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1253 - accuracy: 0.9549 - val_loss: 0.6759 - val_accuracy: 0.8390\n",
            "Epoch 163/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1264 - accuracy: 0.9544 - val_loss: 0.6441 - val_accuracy: 0.8396\n",
            "Epoch 164/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1219 - accuracy: 0.9566 - val_loss: 0.6780 - val_accuracy: 0.8325\n",
            "Epoch 165/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1217 - accuracy: 0.9562 - val_loss: 0.6772 - val_accuracy: 0.8380\n",
            "Epoch 166/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1196 - accuracy: 0.9579 - val_loss: 0.7410 - val_accuracy: 0.8261\n",
            "Epoch 167/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1191 - accuracy: 0.9567 - val_loss: 0.6575 - val_accuracy: 0.8416\n",
            "Epoch 168/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1157 - accuracy: 0.9578 - val_loss: 0.6417 - val_accuracy: 0.8427\n",
            "Epoch 169/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1149 - accuracy: 0.9589 - val_loss: 0.7725 - val_accuracy: 0.8279\n",
            "Epoch 170/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1149 - accuracy: 0.9587 - val_loss: 0.6564 - val_accuracy: 0.8496\n",
            "Epoch 171/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1129 - accuracy: 0.9581 - val_loss: 0.7057 - val_accuracy: 0.8429\n",
            "Epoch 172/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1121 - accuracy: 0.9596 - val_loss: 0.6600 - val_accuracy: 0.8428\n",
            "Epoch 173/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1111 - accuracy: 0.9603 - val_loss: 0.6706 - val_accuracy: 0.8482\n",
            "Epoch 174/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1052 - accuracy: 0.9623 - val_loss: 0.6612 - val_accuracy: 0.8458\n",
            "Epoch 175/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1058 - accuracy: 0.9622 - val_loss: 0.7434 - val_accuracy: 0.8281\n",
            "Epoch 176/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1061 - accuracy: 0.9620 - val_loss: 0.6825 - val_accuracy: 0.8402\n",
            "Epoch 177/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1028 - accuracy: 0.9634 - val_loss: 0.6870 - val_accuracy: 0.8443\n",
            "Epoch 178/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1007 - accuracy: 0.9646 - val_loss: 0.7832 - val_accuracy: 0.8266\n",
            "Epoch 179/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.1017 - accuracy: 0.9640 - val_loss: 0.8030 - val_accuracy: 0.8240\n",
            "Epoch 180/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0984 - accuracy: 0.9638 - val_loss: 0.6830 - val_accuracy: 0.8424\n",
            "Epoch 181/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0938 - accuracy: 0.9660 - val_loss: 0.6691 - val_accuracy: 0.8492\n",
            "Epoch 182/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0942 - accuracy: 0.9667 - val_loss: 0.7792 - val_accuracy: 0.8362\n",
            "Epoch 183/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0968 - accuracy: 0.9647 - val_loss: 0.7408 - val_accuracy: 0.8380\n",
            "Epoch 184/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0960 - accuracy: 0.9663 - val_loss: 0.7050 - val_accuracy: 0.8448\n",
            "Epoch 185/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0930 - accuracy: 0.9666 - val_loss: 0.6987 - val_accuracy: 0.8438\n",
            "Epoch 186/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0898 - accuracy: 0.9677 - val_loss: 0.6791 - val_accuracy: 0.8473\n",
            "Epoch 187/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0925 - accuracy: 0.9674 - val_loss: 0.6896 - val_accuracy: 0.8467\n",
            "Epoch 188/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0904 - accuracy: 0.9680 - val_loss: 0.7391 - val_accuracy: 0.8413\n",
            "Epoch 189/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0848 - accuracy: 0.9699 - val_loss: 0.8033 - val_accuracy: 0.8263\n",
            "Epoch 190/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0848 - accuracy: 0.9700 - val_loss: 0.7282 - val_accuracy: 0.8396\n",
            "Epoch 191/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0874 - accuracy: 0.9683 - val_loss: 0.6914 - val_accuracy: 0.8448\n",
            "Epoch 192/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0801 - accuracy: 0.9721 - val_loss: 0.7892 - val_accuracy: 0.8370\n",
            "Epoch 193/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0836 - accuracy: 0.9696 - val_loss: 0.7216 - val_accuracy: 0.8452\n",
            "Epoch 194/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0850 - accuracy: 0.9696 - val_loss: 0.7050 - val_accuracy: 0.8454\n",
            "Epoch 195/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0787 - accuracy: 0.9712 - val_loss: 0.7192 - val_accuracy: 0.8444\n",
            "Epoch 196/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0798 - accuracy: 0.9712 - val_loss: 0.7200 - val_accuracy: 0.8438\n",
            "Epoch 197/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0773 - accuracy: 0.9722 - val_loss: 0.7242 - val_accuracy: 0.8405\n",
            "Epoch 198/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0774 - accuracy: 0.9721 - val_loss: 0.7259 - val_accuracy: 0.8409\n",
            "Epoch 199/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0773 - accuracy: 0.9730 - val_loss: 0.7186 - val_accuracy: 0.8470\n",
            "Epoch 200/200\n",
            "391/391 [==============================] - 13s 33ms/step - loss: 0.0760 - accuracy: 0.9725 - val_loss: 0.7377 - val_accuracy: 0.8433\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f482e4eff98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOE5QPW8KSHJ"
      },
      "source": [
        "若依照以上範例的設定，經過 20 輪訓練後，DNN 可以到達約 40% 的正確率，而 CNN 可以達到約 50% 的正確率。請更改 DNN 或是 CNN 的架構來改善模型。提示：\n",
        "\n",
        "1. 調整訓練輪數(`epochs`)\n",
        "2. 調整批次大小(`batch_size`)\n",
        "3. 調整學習速率(`learning_rate`)\n",
        "4. 增加層數\n",
        "5. 調整每層的神經元數量"
      ]
    }
  ]
}